---
layout:     post
title:      kmeans++ algorithm
keywords:   kmeans++
category:   [machine-learning]
tags:       [machine-learning-basic]
---

**前言：**
传统的kmeans的初始中心点是随机选取的，容易陷入局部最优，kmeans++算法就是为了解决问题。它提出了一个更好的
中心点初始化方式。

## 算法简介

 k-means++算法选择初始seeds的基本思想就是：

	初始的聚类中心之间的相互距离要尽可能的远。

**算法步骤：**

（1）从输入的数据点集合中随机选择一个点作为第一个聚类中心
（2）对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
（3）选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
（4）重复2和3直到k个聚类中心被选出来
（5）利用这k个初始的聚类中心来运行标准的k-means算法

**第三步的具体做法：**
- 先从我们的数据库随机挑一个随机点当“种子点”。
-  对于每个样本点x，我们都计算其和最近的一个“种子点”的距离$D(x)$并保存在一个数组里，然后把这些距离加起来得到$Sum(D(x))$。
-  然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值 $Random$，然后用$Random -= D(x)$， 直到其$Random <= 0$，此时的点( D(x) )就是下一个“种子点”。
其中，$Random = Sum(D(x)) * 乘以0至1之间的一个小数$

之所以取一个能落在Sum(D(x))中的值是因为，Random是随机的，那么他有更大的机率落在D(x)值较大的区域里。如下图，Random有更大的机率落在D(x3)中。

	Random -= D(x) 的意义在于找出 当前Random到底落在了哪个区间。

![](/images/machine-learning/kmeans++.png)

	从上图可以看出，假设Random落在D(x3)这个区间内，“然后用Random -= D(x),直到Random<=0"此时找到的点就是D(x3)，就是这步的中心点。



## 代码：
	http://rosettacode.org/wiki/K-means%2B%2B_clustering

c版本的编译不过。我修改了一下。但是还有疑问：
（1）M_PI在math.h中，为什么包含了后，还是提示未定义？？
（2）inline类型的函数，都是提醒未定义。（用的是c99，估计和他有关）

**修改后的C代码：**

```
/*
usage:
deal with 2-dimension data's kmeans++.
*/

#include <stdio.h>
#include <stdlib.h>
#include <math.h>

#define M_PI 3.1415926

typedef struct { double x, y; int group; } point_t, *point;
 
double randf(double m)
{
	return m * rand() / (RAND_MAX - 1.);
}
 
point gen_xy(int count, double radius)
{
	double ang, r;
	point p, pt = malloc(sizeof(point_t) * count);
 
	/* note: this is not a uniform 2-d distribution */
	for (p = pt + count; p-- > pt;) {
		ang = randf(2 * M_PI);
		r = randf(radius);
		p->x = r * cos(ang);
		p->y = r * sin(ang);
	}
	return pt;
}
 
double dist2(point a, point b)
{
	double x = a->x - b->x, y = a->y - b->y;
	return x*x + y*y;
}
 
int nearest(point pt, point cent, int n_cluster, double *d2)
{
	int i, min_i;
	point c;
	double d, min_d;

//#	define for_n for (c = cent, i = 0; i < n_cluster; i++, c++)
	//for_n {
	for (c = cent, i = 0; i < n_cluster; i++, c++){
		min_d = HUGE_VAL;
		min_i = pt->group;
		//for_n {
		for (c = cent, i = 0; i < n_cluster; i++, c++){
			if (min_d > (d = dist2(c, pt))) {
				min_d = d; min_i = i;
			}
		}
	}
	if (d2) *d2 = min_d;
	return min_i;
}
 
void kpp(point pts, int len, point cent, int n_cent)
{
//#	define for_len for (j = 0, p = pts; j < len; j++, p++)
	int i, j;
	int n_cluster;
	double sum;
	double *d = malloc(sizeof(double) * len);
 
	point p, c;
	cent[0] = pts[ rand() % len ];

	for (n_cluster = 1; n_cluster < n_cent; n_cluster++) {
		sum = 0;
		//for_len {
		for (j = 0, p = pts; j < len; j++, p++){
			nearest(p, cent, n_cluster, d + j);
			sum += d[j];
		}
		sum = randf(sum);
		//for_len {
		for (j = 0, p = pts; j < len; j++, p++){
			if ((sum -= d[j]) > 0) continue;
			cent[n_cluster] = pts[j];
			break;
		}
	}
	//for_len p->group = nearest(p, cent, n_cluster, 0);
	for (j = 0, p = pts; j < len; j++, p++) p->group = nearest(p, cent, n_cluster, 0);
	free(d);
}
 
point lloyd(point pts, int len, int n_cluster, int* iter)
{
	int i, j, min_i;
	int changed;
 	
 	int nIter = 0;
	point cent = malloc(sizeof(point_t) * n_cluster);
	point p, c;
 
	/* assign init grouping randomly */
	//for_len p->group = j % n_cluster;
 
	/* or call k++ init */
	kpp(pts, len, cent, n_cluster);
 
	do {
		/* group element for centroids are used as counters */
		nIter ++; 

		for (c = cent, i = 0; i < n_cluster; i++, c++){
			c->group = 0; 
			c->x = c->y = 0;
		}
		

		for (j = 0, p = pts; j < len; j++, p++){
			c = cent + p->group;
			c->group++;
			c->x += p->x; c->y += p->y;
		}
		for (c = cent, i = 0; i < n_cluster; i++, c++){
		 	c->x /= c->group; 
		 	c->y /= c->group; 
		}
 
		changed = 0;

		/* find closest centroid of each point */
		for (j = 0, p = pts; j < len; j++, p++){
			min_i = nearest(p, cent, n_cluster, 0);
			if (min_i != p->group) {
				changed++;
				p->group = min_i;
			}
		}
	} while (changed > (len >> 10)); /* stop when 99.9% of points are good */

	for (c = cent, i = 0; i < n_cluster; i++, c++){
		c->group = i; 
	}

 	*iter = nIter;
	return cent;
}
 

#define POINT_NUM 200
#define K 11

void print_matrix(point arr, int len){
	int i;
	for(i=0; i<len; i++){
		printf("(%f, %f)\n", arr[i].x, arr[i].y);
	}
}

int main()
{
	int i;
	int iter = 0;
	point v = gen_xy(POINT_NUM, 10);
	point c = lloyd(v, POINT_NUM, K, &iter);

	printf("iter time: %d\n", iter);
	print_matrix(c, K);

	// free(v); free(c);
	return 0;
}

```

## 总结：
1. kmeans++选取的初始中心点， 相比随机生成的，可以使迭代次数降低。但是k的值还是需要用户输入。
2. 它的并行化呢？（ICPP2016： Parallel k-means++ for Multiple Shared-Memory）它把kmeans++在多核cpu，GPU和Cray XMT platform上并行化。
-》读论文看他的创新点在哪里。
 



## 参考文献：（懂啦，把第三步的操作具体步骤整理一下）
	http://blog.csdn.net/loadstar_kun/article/details/39450615

