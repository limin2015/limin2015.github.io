---
layout: post
title:  LSTM introduction
keywords: LSTM
categories : NLP
tags:
  - NLP
---

# lstm的串行代码，计算模式分析

https://www.jianshu.com/p/4b4701beba92



这个博客讲的特别好：整理一下。

https://www.jianshu.com/p/2aca6e8ac7c8


LSTM的第一步是决定我们要从细胞状态中丢弃什么信息。 该决定由被称为“忘记门”的Sigmoid层实现。它查看ht-1(前一个输出)和xt(当前输入)，并为单元格状态Ct-1(上一个状态)中的每个数字输出0和1之间的数字。1代表完全保留，而0代表彻底删除。

公式：  ![](/images/NLP/lstm-1.png)


下一步是决定我们要在细胞状态中存储什么信息。 这部分分为两步。 首先，称为“输入门层”的Sigmoid层决定了我们将更新哪些值。 接下来一个tanh层创建候选向量Ct,该向量将会被加到细胞的状态中。 在下一步中，我们将结合这两个向量来创建更新值。

公式：  ![](/images/NLP/lstm-2.png)



现在是时候去更新上一个状态值Ct−1了，将其更新为Ct。签名的步骤以及决定了应该做什么，我们只需实际执行即可。
我们将上一个状态值乘以ft，以此表达期待忘记的部分。之后我们将得到的值加上 it∗C̃ t。这个得到的是新的候选值， 按照我们决定更新每个状态值的多少来衡量.

公式：  ![](/images/NLP/lstm-3.png)



最后，我们需要决定我们要输出什么。 此输出将基于我们的细胞状态，但将是一个过滤版本。 首先，我们运行一个sigmoid层，它决定了我们要输出的细胞状态的哪些部分。 然后，我们将单元格状态通过tanh（将值规范化到-1和1之间），并将其乘以Sigmoid门的输出，至此我们只输出了我们决定的那些部分。

公式：  ![](/images/NLP/lstm-4.png)


# 计算模式分析：

本质上，都是一些gemv和向量加，向量做sigmoid，向量做tanh这些操作的组合。

想一下cg法的并行优化方法：感觉可以用到lstm中来。





