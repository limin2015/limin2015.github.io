---
layout: post
title:  CUDA性能优化-存储体冲突
keywords: CUDA
categories : [CUDA]
tags : [CUDA编程]
---

TODO: 介绍cuda性能优化的几个重要的点。及其比较新的一些feature。
（寒假放假前，整理完成！！）


# 存储体冲突是什么？

bank-conflict:


#如何减少存储体冲突？


## warp深度解析：

下面这个blog分析的非常好：

http://blog.163.com/wujiaxing009@126/blog/static/71988399201701224540201/



## 关于bank conflict的解释：

![](/images/cuda/bank-conflict.png)

一个块内的线程，最好相邻线程访问的是相邻的内存。否则容易bank conflict。


下面的介绍很好：（good！！）

http://blog.csdn.net/u013701860/article/details/50253343



## global-mem的合并访问：不大很明白

是说：相邻线程访问相邻的元素吗？
比如任务分配的时候，采用的方式是0号第0个元素，1号第1个元素，然后round-robin方式继续进行下去。


## block间共享数据的时候，必须通过global-mem吗？

是的！


## cuda分块时的线程索引，位置确定。我有点晕了的！！





## cuda中使用 #pragma unroll 是做什么的？？

显示的做循环展开。

## 寄存器变量的使用：

http://blog.csdn.net/tiemaxiaosu/article/details/52932455

http://blog.csdn.net/tiemaxiaosu/article/details/52956150

http://blog.csdn.net/tiemaxiaosu/article/details/52932455

http://blog.csdn.net/u013507368/article/details/43370423

http://blog.csdn.net/Bruce_0712/article/details/65664840





## texture-memory：

只读的；（必须全局声明！！）

http://john-waindinger.blog.163.com/blog/static/232830112201472694143498/

这个介绍不错：


http://blog.csdn.net/zhangfuliang123/article/details/76528075


## 关于汇编（ptx等）相关的：

asfermi：我把它下载并放到了124服务器上，编译通过了的。

https://code.google.com/archive/p/asfermi

它的使用等：

https://code.google.com/archive/p/asfermi/wikis/CodeExample.wiki




## PTX代码的学习：

**官方文档**：

http://docs.nvidia.com/cuda/parallel-thread-execution/index.html#axzz3x1T0rHkf

别人的博客：

http://blog.csdn.net/fishseeker/article/details/75214167
http://blog.csdn.net/litdaguang/article/details/50505885
http://blog.csdn.net/litdaguang/article/details/50505885
http://blog.csdn.net/Canhui_WANG/article/details/52892676


SASS指令集。NVASM和cuobjdump上有一些，但感觉不全。NV没有公布过assembler。Fermi上可以去看开源的asfermi，里面讲了一些。Maxwell可以去查maxas，其他的应该没了。


有空的时候看这个博客的整理：（**翻译手册**）

https://cloud.tencent.com/developer/article/1016284
https://cloud.tencent.com/developer/article/1016295




## float2和float4向量数据类型是什么鬼？？




## launch bounds  及  if (__CUDA_ARCH__ >= 200)这句是什么意思：





## gpu的任务并行和数据并行分别指什么？

For Ivy Bridge and MIC, the task parallelism is achieved by utilizing
multiple hardware threads. For MIC, the data parallelism benefits
from on-core VPU (Vector Processing Unit) and SIMD. 

For Fermi
and Kepler, the task parallelism comes from the independent warps
that are executed **by different SMs**（\red{really}）. 
In each warp, the data-parallelism
is achieved by the computations performed by the different CUDA
cores(SP) within the SM. In order to get satisfactory performance, fully
utilizing the two-level parallelism and improving the occupancy rate
of the computing resources are crucial.


## gpu的指令并行指的是什么？







## 什么时候使用cudaThreadSynchronize()？

等待kernel执行完毕。
cpu端发起kernel<<< >>>之后（异步的），就可以继续执行下面的程序（该程序段与gpu加速的没有依赖关系）。使用
cudaThreadSynchronize()后，cpu会等待gpu的结果返回后，才继续执行下面的指令。





## 一篇博客，写如何改善gpu性能的，很有用

https://www.cnblogs.com/Jnshushi99/p/4711060.html

https://www.cnblogs.com/ghl_carmack/p/4107042.html


## GTC2017：（去看看！！）

http://www.gputechconf.com/gtcnew/on-demand-gtc.php

