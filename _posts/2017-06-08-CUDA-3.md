---
layout: post
title:  CUDA编程实践-3-CUDA by Example
keywords: reading-cuda-books
categories : [CUDA]
tags : [CUDA编程]
---

任务：

《CUDA by Example》读书笔记（from p21）

计划：

共237页：一天读10页即可。

# eg-1：summing two vectors

程序在服务器上的地址（124服务器）：

/home/limin/exer/cuda_by_example/chapter04/add_loop_long.cu 

知识点：

1.块数最多65535.块内线程数最多512（有的事1024）。

2.下面的核心代码可以接受任意大小的N，只要RAM里面装得下即可。

**核心代码**：

```
#define N (1024*33)

//kernel函数
__global__ void add_3( int *a, int *b, int *c ) {
	int tid = blockIdx.x * blockDim.x + threadIdx.x;
 	while (tid < N) {
 		c[tid] = a[tid] + b[tid];
 		tid += blockDim.x * gridDim.x;
	}
}
//发起kernel的语句：
add_3<<<32,512>>>( dev_a, dev_b, dev_c );
```

思考一个问题：

（1）针对上面的函数，如何设置最优块数，和块内线程数？书的后半部分有。

（2）用cuda的event和我的dtime测得kernel的时间不一样。后者小很多。why？？


# eg-2：JulIA set
这个例子，我收获很大。因为它类似于一个应用。感觉优化时的视角跟优化一个kernel的感觉很不一样。
每个线程做的事情，就像我在sw上编程时一样，可以为任意复杂的运算，当然前提是不涉及没法实现的通信。

CPU串行代码：
p47

GPU并行代码：



# eg-3：dot操作（blas-1级的函数）
share-memory的使用

黏贴代码处：

```
#define imin(a,b) (a<b?a:b)

const int N = 33 * 1024;
const int threadsPerBlock = 256;
const int blocksPerGrid =
            imin( 32, (N+threadsPerBlock-1) / threadsPerBlock );
//kernel函数
__global__ void dot( float *a, float *b, float *c ) {
    __shared__ float cache[threadsPerBlock];
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    int cacheIndex = threadIdx.x;

    float   temp = 0;
    while (tid < N) {
        temp += a[tid] * b[tid];
        tid += blockDim.x * gridDim.x;
    }
    
    // set the cache values
    cache[cacheIndex] = temp;
    
    // synchronize threads in this block
    __syncthreads();

    // for reductions, threadsPerBlock must be a power of 2
    // because of the following code
    int i = blockDim.x/2;
    while (i != 0) {
        if (cacheIndex < i)
            cache[cacheIndex] += cache[cacheIndex + i];
        __syncthreads();
        i /= 2;
    }

    if (cacheIndex == 0)
        c[blockIdx.x] = cache[0];
}

//主函数
int main( void ) {
    float   *a, *b, c, *partial_c;
    float   *dev_a, *dev_b, *dev_partial_c;

    // allocate memory on the cpu side
    a = (float*)malloc( N*sizeof(float) );
    b = (float*)malloc( N*sizeof(float) );
    partial_c = (float*)malloc( blocksPerGrid*sizeof(float) );

    // allocate the memory on the GPU
    HANDLE_ERROR( cudaMalloc( (void**)&dev_a,
                              N*sizeof(float) ) );
    HANDLE_ERROR( cudaMalloc( (void**)&dev_b,
                              N*sizeof(float) ) );
    HANDLE_ERROR( cudaMalloc( (void**)&dev_partial_c,
                              blocksPerGrid*sizeof(float) ) );

    // fill in the host memory with data
    for (int i=0; i<N; i++) {
        a[i] = i;
        b[i] = i*2;
    }

    // copy the arrays 'a' and 'b' to the GPU
    HANDLE_ERROR( cudaMemcpy( dev_a, a, N*sizeof(float),
                              cudaMemcpyHostToDevice ) );
    HANDLE_ERROR( cudaMemcpy( dev_b, b, N*sizeof(float),
                              cudaMemcpyHostToDevice ) ); 

    dot<<<blocksPerGrid,threadsPerBlock>>>( dev_a, dev_b,
                                            dev_partial_c );

    // copy the array 'c' back from the GPU to the CPU
    HANDLE_ERROR( cudaMemcpy( partial_c, dev_partial_c,
                              blocksPerGrid*sizeof(float),
                              cudaMemcpyDeviceToHost ) );

    // finish up on the CPU side
    c = 0;
    for (int i=0; i<blocksPerGrid; i++) {
        c += partial_c[i];
    }

    #define sum_squares(x)  (x*(x+1)*(2*x+1)/6)
    printf( "Does GPU value %.6g = %.6g?\n", c,
             2 * sum_squares( (float)(N - 1) ) );

    // free memory on the gpu side
    HANDLE_ERROR( cudaFree( dev_a ) );
    HANDLE_ERROR( cudaFree( dev_b ) );
    HANDLE_ERROR( cudaFree( dev_partial_c ) );

    // free memory on the cpu side
    free( a );
    free( b );
    free( partial_c );
}
```

p90页的没看。
TODO

# eg-5：constant memory的使用

知识点：

1.host拷到constant memory时，需要使用函数：cudaMemcpyToSymbol（）。host拷到global memory使用函数：cudaMemcpy()。两者参数一致。

2.constant memory的大小是64KB。

3.为什么从constant memory读数，比global memory要快？

• A single read from constant memory can be broadcast to other “nearby”
threads, effectively saving up to 15 reads.（广播给half-warp）
===》If every thread in a half-warp requests data from the same address in constant memory, your GPU will generate only a single read request and subsequently broadcast
the data to every thread。


• Constant memory is cached, so consecutive reads of the same address will not
incur any additional memory traffc.
===》 constant memory cache


p107页解释的非常好：什么时候要使用constant memory？

	答：当正好半个warp的线程，使用的是同一个address的constant memory的值时。如果不是的话，使用constant就不会又加速啦。反倒会减慢。


疑问：const memory只能是全局变量吗？

	答：定义常数存储器时，需要将其定义在所有函数之外，作用于整个文件。

# eg-6：使用event测试时间。

1.kernel之前：

```
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);
cudaEventRecord( start, 0 );
```
2.kernel之后：

```
cudaEventRecord(stop, 0);
cudaEventSynchronize(stop);
float time;
cudaEventElapsedTime(&time, start, stop);
printf("time used: %lf ms\n", time);
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

# eg-7：texture memory
知识点：

1.只适用于以下的数据访问模式（有加速效果）



看到112页啦。
TOOD



# gpu中的task parallel：stream
#### 1.申请host memory的两种方法：

（1）使用c中提供的：malloc（）===》pageable host memory

（2）使用cuda运行时提供的API：cudaHostAlloc() ===》a buffer of page-locked host memory（pinned memory）

区别：

使用（2）申请的host memory，The operating system guarantees us that it will never page this memory out to disk, which ensures its residency in physical memory。
因为它的物理地址是知道的，所以，the GPU can then use direct memory
access (DMA) to copy data to or from the host。

但是，（2）也有缺点。因为它直接申请的就是物理地址，而机器的物理空间是有限的，故当超过这个限制的时候，就会出错。（只适用于对host memory需求不是那么高的时候）

使用（1）的申请的host memory，first from a pageable system buffer to a page-locked “staging” buffer（虚拟地址到物理地址的转换；走PCIe总线） and then from the page-locked system buffer to the GPU（ front-side bus）。

#### 2. cudaHostAlloc()的用法

（1）every malloc() needs a free(), and every cudaHostAlloc() needs
a cudaFreeHost()。

（2）函数接口：
cudaHostAlloc( (void**)&a, size * sizeof( *a ),cudaHostAllocDefault ) );

#### 3. a benchmark cudaMemcpy() performance with both pageable and page-locked memory

*malloc的方式：*
```
float cuda_malloc_test( int size, bool up ) {
    cudaEvent_t     start, stop;
    int             *a, *dev_a;
    float           elapsedTime;

    HANDLE_ERROR( cudaEventCreate( &start ) );
    HANDLE_ERROR( cudaEventCreate( &stop ) );

    a = (int*)malloc( size * sizeof( *a ) );
    HANDLE_NULL( a );
    HANDLE_ERROR( cudaMalloc( (void**)&dev_a,
                              size * sizeof( *dev_a ) ) );

    HANDLE_ERROR( cudaEventRecord( start, 0 ) );
    for (int i=0; i<100; i++) {
        if (up)
            HANDLE_ERROR( cudaMemcpy( dev_a, a,
                                  size * sizeof( *dev_a ),
                                  cudaMemcpyHostToDevice ) );
        else
            HANDLE_ERROR( cudaMemcpy( a, dev_a,
                                  size * sizeof( *dev_a ),
                                  cudaMemcpyDeviceToHost ) );
    }
    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );
    HANDLE_ERROR( cudaEventSynchronize( stop ) );
    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,
                                        start, stop ) );

    free( a );
    HANDLE_ERROR( cudaFree( dev_a ) );
    HANDLE_ERROR( cudaEventDestroy( start ) );
    HANDLE_ERROR( cudaEventDestroy( stop ) );

    return elapsedTime;
}
```

*cudaHostAlloc的方式：*
```
float cuda_host_alloc_test( int size, bool up ) {
    cudaEvent_t     start, stop;
    int             *a, *dev_a;
    float           elapsedTime;

    HANDLE_ERROR( cudaEventCreate( &start ) );
    HANDLE_ERROR( cudaEventCreate( &stop ) );

    HANDLE_ERROR( cudaHostAlloc( (void**)&a,
                                 size * sizeof( *a ),
                                 cudaHostAllocDefault ) );
    HANDLE_ERROR( cudaMalloc( (void**)&dev_a,
                              size * sizeof( *dev_a ) ) );

    HANDLE_ERROR( cudaEventRecord( start, 0 ) );
    for (int i=0; i<100; i++) {
        if (up)
            HANDLE_ERROR( cudaMemcpy( dev_a, a,
                                  size * sizeof( *a ),
                                  cudaMemcpyHostToDevice ) );
        else
            HANDLE_ERROR( cudaMemcpy( a, dev_a,
                                  size * sizeof( *a ),
                                  cudaMemcpyDeviceToHost ) );
    }
    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );
    HANDLE_ERROR( cudaEventSynchronize( stop ) );
    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,
                                        start, stop ) );

    HANDLE_ERROR( cudaFreeHost( a ) );
    HANDLE_ERROR( cudaFree( dev_a ) );
    HANDLE_ERROR( cudaEventDestroy( start ) );
    HANDLE_ERROR( cudaEventDestroy( stop ) );

    return elapsedTime;
}
```


*main函数（host函数）：*
```
int main( void ) {
    float           elapsedTime;
    float           MB = (float)100*SIZE*sizeof(int)/1024/1024;


    // try it with cudaMalloc
    elapsedTime = cuda_malloc_test( SIZE, true );
    printf( "Time using cudaMalloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy up:  %3.1f\n",
            MB/(elapsedTime/1000) );

    elapsedTime = cuda_malloc_test( SIZE, false );
    printf( "Time using cudaMalloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy down:  %3.1f\n",
            MB/(elapsedTime/1000) );

    // now try it with cudaHostAlloc
    elapsedTime = cuda_host_alloc_test( SIZE, true );
    printf( "Time using cudaHostAlloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy up:  %3.1f\n",
            MB/(elapsedTime/1000) );

    elapsedTime = cuda_host_alloc_test( SIZE, false );
    printf( "Time using cudaHostAlloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy down:  %3.1f\n",
            MB/(elapsedTime/1000) );
}
```


# 疑问及解答

问题-1：

thrust只能在main函数这种host函数中调用吗，还是可以在kernel函数中个调用？


# reference
1.[CUDA by Example]()
