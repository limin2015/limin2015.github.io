---
layout:     post
title:      最大似然估计、EM算法、正则化方法
keywords:   EM算法
category:   [machine-learning]
tags:       [machine-learning-basic]
---


本节的内容需要整理！！！





# 最大似然估计


如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然法估计参数。

李航的统计机器学习的p87.


# EM算法



EM算法指的是最大期望算法（Expectation Maximization Algorithm，又译期望最大化算法），是一种迭代算法，用于含有隐变量（latent variable）的概率参数模型的最大似然估计或极大后验概率估计。



## 原理

先看李航的统计机器学习的p156，讲的很清晰，并带有一个例子。

包含两步：

E步：求期望；

M步：求极大化；



## 应用场景



## 一个简单的实现（python）



## opencv中的代码





# 正则化方法

[L0，L1， L2正则化](http://blog.csdn.net/zouxy09/article/details/24971995)
中说：

- 第一项Loss函数，如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是exp-Loss，那就是牛逼的 Boosting了；如果是log-Loss，那就是Logistic Regression了。（需要再想想！！）

- 迹范数、Frobenius范数和核范数


[核范数与规则项参数选择](http://blog.csdn.net/zouxy09/article/details/24972869)






# 参考资料

1. [EM-from-weixin](https://mp.weixin.qq.com/s/wIGr79NbUnYnk0ifqqRH7A)

2. [PCA-SVD](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247486147&idx=1&)sn=ae6c99fbd00c9ea7de31d691472dd21c&chksm=ebb43217dcc3bb01fe10d278b51fd44777a6c00ce95107698d6ab4df021856b3885154c07b0b&scene=21#wechat_redirect