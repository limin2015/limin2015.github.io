---
layout: post
title:  CUDA编程实践-4-CUDA by Example
keywords: reading-cuda-books
categories : [CUDA]
tags : [CUDA编程]
---

因为这本书读起来很快，故，只整理了2个文档。使用了大约一周时间。


# section-9： 原子操作

## 常用的原子操作指令列表（API）

**ref：** http://www.cnblogs.com/biglucky/p/4283476.html

    atomicAdd( &data, 1 );//data地址处的值原子加1。1也可以换做其他的数。
    atomicSub( &data, 1 );//data地址处的值原子减1。1也可以换做其他的数。

    //计算 (*mutex == 0 ? 1 : *mutex)，返回*mutex
    __device__ void lock( void ) {
      while( atomicCAS( mutex, 0, 1 ) != 0 );//metex不等于0时，被阻塞在这里啦。
    }

    atomicExch( mutex, 1 );//*mutex = 1; mutex是一个指针:int * mutex。

## histgram的例子

**ver-1:cpu版本：**

```
 for (int i=0; i<SIZE; i++)
        histo[buffer[i]]++;
```

**ver-2：gpu-global-memory版本：**

```
__global__ void histo_kernel( unsigned char *buffer,
                              long size,
                              unsigned int *histo ) {
    // calculate the starting index and the offset to the next
    // block that each thread will be processing
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    int stride = blockDim.x * gridDim.x;
    while (i < size) {
        atomicAdd( &histo[buffer[i]], 1 );
        i += stride;
    }
}
```

**ver-3：gpu-share-memory版本：**
```
__global__ void histo_kernel( unsigned char *buffer,
                              long size,
                              unsigned int *histo ) {

    // clear out the accumulation buffer called temp
    // since we are launched with 256 threads, it is easy
    // to clear that memory with one write per thread
    __shared__  unsigned int temp[256];
    temp[threadIdx.x] = 0;
    __syncthreads();

    // calculate the starting index and the offset to the next
    // block that each thread will be processing
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    int stride = blockDim.x * gridDim.x;
    while (i < size) {
        atomicAdd( &temp[buffer[i]], 1 );
        i += stride;
    }
    // sync the data from the above writes to shared memory
    // then add the shared memory values to the values from
    // the other thread blocks using global memory
    // atomic adds
    // same as before, since we have 256 threads, updating the
    // global histogram is just one write per thread!
    __syncthreads();
    atomicAdd( &(histo[threadIdx.x]), temp[threadIdx.x] );
}
```

## hashtable的例子(good)
TODO



# section-10： gpu中的task parallel：stream

## 1.申请host memory的两种方法：

（1）使用c中提供的：malloc（）===》pageable host memory

（2）使用cuda运行时提供的API：cudaHostAlloc() ===》a buffer of page-locked host memory（pinned memory）

**区别：** 

使用（2）申请的host memory，The operating system guarantees us that it will never page this memory out to disk, which ensures its residency in physical memory。
因为它的物理地址是知道的，所以，the GPU can then use direct memory
access (DMA) to copy data to or from the host。

但是，（2）也有缺点。因为它直接申请的就是物理地址，而机器的物理空间是有限的，故当超过这个限制的时候，就会出错。（只适用于对host memory需求不是那么高的时候）

使用（1）的申请的host memory，first from a pageable system buffer to a page-locked “staging” buffer（虚拟地址到物理地址的转换；走PCIe总线） and then from the page-locked system buffer to the GPU（ front-side bus）。

## 2. cudaHostAlloc()的用法

（1）every malloc() needs a free(), and every cudaHostAlloc() needs
a cudaFreeHost()。

（2）函数接口：
cudaHostAlloc( (void**)&a, size * sizeof( *a ),cudaHostAllocDefault ) );

## 3. a benchmark cudaMemcpy() performance with both pageable and page-locked memory

*malloc的方式：* 

```
float cuda_malloc_test( int size, bool up ) {
    cudaEvent_t     start, stop;
    int             *a, *dev_a;
    float           elapsedTime;

    HANDLE_ERROR( cudaEventCreate( &start ) );
    HANDLE_ERROR( cudaEventCreate( &stop ) );

    a = (int*)malloc( size * sizeof( *a ) );
    HANDLE_NULL( a );
    HANDLE_ERROR( cudaMalloc( (void**)&dev_a,
                              size * sizeof( *dev_a ) ) );

    HANDLE_ERROR( cudaEventRecord( start, 0 ) );
    for (int i=0; i<100; i++) {
        if (up)
            HANDLE_ERROR( cudaMemcpy( dev_a, a,
                                  size * sizeof( *dev_a ),
                                  cudaMemcpyHostToDevice ) );
        else
            HANDLE_ERROR( cudaMemcpy( a, dev_a,
                                  size * sizeof( *dev_a ),
                                  cudaMemcpyDeviceToHost ) );
    }
    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );
    HANDLE_ERROR( cudaEventSynchronize( stop ) );
    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,
                                        start, stop ) );

    free( a );
    HANDLE_ERROR( cudaFree( dev_a ) );
    HANDLE_ERROR( cudaEventDestroy( start ) );
    HANDLE_ERROR( cudaEventDestroy( stop ) );

    return elapsedTime;
}
```

*cudaHostAlloc的方式： * 

```
float cuda_host_alloc_test( int size, bool up ) {
    cudaEvent_t     start, stop;
    int             *a, *dev_a;
    float           elapsedTime;

    HANDLE_ERROR( cudaEventCreate( &start ) );
    HANDLE_ERROR( cudaEventCreate( &stop ) );

    HANDLE_ERROR( cudaHostAlloc( (void**)&a,
                                 size * sizeof( *a ),
                                 cudaHostAllocDefault ) );
    HANDLE_ERROR( cudaMalloc( (void**)&dev_a,
                              size * sizeof( *dev_a ) ) );

    HANDLE_ERROR( cudaEventRecord( start, 0 ) );
    for (int i=0; i<100; i++) {
        if (up)
            HANDLE_ERROR( cudaMemcpy( dev_a, a,
                                  size * sizeof( *a ),
                                  cudaMemcpyHostToDevice ) );
        else
            HANDLE_ERROR( cudaMemcpy( a, dev_a,
                                  size * sizeof( *a ),
                                  cudaMemcpyDeviceToHost ) );
    }
    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );
    HANDLE_ERROR( cudaEventSynchronize( stop ) );
    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,
                                        start, stop ) );

    HANDLE_ERROR( cudaFreeHost( a ) );
    HANDLE_ERROR( cudaFree( dev_a ) );
    HANDLE_ERROR( cudaEventDestroy( start ) );
    HANDLE_ERROR( cudaEventDestroy( stop ) );

    return elapsedTime;
}
```


*main函数（host函数）： *

```
int main( void ) {
    float           elapsedTime;
    float           MB = (float)100*SIZE*sizeof(int)/1024/1024;


    // try it with cudaMalloc
    elapsedTime = cuda_malloc_test( SIZE, true );
    printf( "Time using cudaMalloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy up:  %3.1f\n",
            MB/(elapsedTime/1000) );

    elapsedTime = cuda_malloc_test( SIZE, false );
    printf( "Time using cudaMalloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy down:  %3.1f\n",
            MB/(elapsedTime/1000) );

    // now try it with cudaHostAlloc
    elapsedTime = cuda_host_alloc_test( SIZE, true );
    printf( "Time using cudaHostAlloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy up:  %3.1f\n",
            MB/(elapsedTime/1000) );

    elapsedTime = cuda_host_alloc_test( SIZE, false );
    printf( "Time using cudaHostAlloc:  %3.1f ms\n",
            elapsedTime );
    printf( "\tMB/s during copy down:  %3.1f\n",
            MB/(elapsedTime/1000) );
}
```


## 4. 单个stream流的使用相关的API：

注意：使用stream时，要先检查： cudaGetDeviceProperties() 是否支持overlap。

	// initialize the stream
	cudaStream_t stream;
	HANDLE_ERROR( cudaStreamCreate( &stream ) );

	// allocate the memory on the GPU
	cudaMalloc( (void**)&dev_a, N * sizeof(int) ) ；
	// allocate page-locked memory, used to stream
	cudaHostAlloc( (void**)&host_a, FULL_DATA_SIZE * sizeof(int),cudaHostAllocDefault )；

	//异步传输数据。
	cudaMemcpyAsync( dev_a, host_a+i, N * sizeof(int), cudaMemcpyHostToDevice, stream)；
	
	//提交kernel函数给gpu执行。
	kernel<<<N/256,256,0,stream>>>( dev_a, dev_b, dev_c );
	
	//把gpu设备上的计算结果传回来
	cudaMemcpyAsync( host_c+i, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost, stream )；

	//流同步。使得stream里面的所有操作执行完，才能执行下面的指令。
	cudaStreamSynchronize( stream )；

	//destroy stream
	cudaStreamDestroy( stream )；

(2)单个流时，流内部的执行顺序时和提交的operation的顺序一致的，先提交的operation先执行。因为是异步的，故，cpu端和gpu端有一部分可以并行，但是很有限（即这种方式可以实现cpu和gpu端的异步并行执行）。

（3）注意：异步传输的时候，只能使用：cudaHostAlloc()申请的host memory

（4）p195：启发：当global memory的不够用时，host的内存不能一下子全部传输到global memory啦，可以分chunk，一个chunk一个chunk的上传。



## 5. 多个stream流的使用相关的API：

其实这就是双buffer的实现（计算与访存掩盖）

两种方式：（后者性能好）

#### 第一种：round-robin方式
	
	*for循环代码（核心代码）：*
	```
	 // now loop over full data, in bite-sized chunks
    for (int i=0; i<FULL_DATA_SIZE; i+= N*2) {
        // copy the locked memory to the device, async
        HANDLE_ERROR( cudaMemcpyAsync( dev_a0, host_a+i,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream0 ) );
        HANDLE_ERROR( cudaMemcpyAsync( dev_b0, host_b+i,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream0 ) );

        kernel<<<N/256,256,0,stream0>>>( dev_a0, dev_b0, dev_c0 );

        // copy the data from device to locked memory
        HANDLE_ERROR( cudaMemcpyAsync( host_c+i, dev_c0,
                                       N * sizeof(int),
                                       cudaMemcpyDeviceToHost,
                                       stream0 ) );


        // copy the locked memory to the device, async
        HANDLE_ERROR( cudaMemcpyAsync( dev_a1, host_a+i+N,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream1 ) );
        HANDLE_ERROR( cudaMemcpyAsync( dev_b1, host_b+i+N,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream1 ) );

        kernel<<<N/256,256,0,stream1>>>( dev_a1, dev_b1, dev_c1 );

        // copy the data from device to locked memory
        HANDLE_ERROR( cudaMemcpyAsync( host_c+i+N, dev_c1,
                                       N * sizeof(int),
                                       cudaMemcpyDeviceToHost,
                                       stream1 ) );
    }
	```

*work schdeling：*

![robin](/images/cuda/round-robin.png)

#### 第二种：breadth-frst方式

*for循环代码（核心代码）：*

```
// now loop over full data, in bite-sized chunks
    for (int i=0; i<FULL_DATA_SIZE; i+= N*2) {
        // enqueue copies of a in stream0 and stream1
        HANDLE_ERROR( cudaMemcpyAsync( dev_a0, host_a+i,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream0 ) );
        HANDLE_ERROR( cudaMemcpyAsync( dev_a1, host_a+i+N,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream1 ) );
        // enqueue copies of b in stream0 and stream1
        HANDLE_ERROR( cudaMemcpyAsync( dev_b0, host_b+i,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream0 ) );
        HANDLE_ERROR( cudaMemcpyAsync( dev_b1, host_b+i+N,
                                       N * sizeof(int),
                                       cudaMemcpyHostToDevice,
                                       stream1 ) );

        // enqueue kernels in stream0 and stream1   
        kernel<<<N/256,256,0,stream0>>>( dev_a0, dev_b0, dev_c0 );
        kernel<<<N/256,256,0,stream1>>>( dev_a1, dev_b1, dev_c1 );

        // enqueue copies of c from device to locked memory
        HANDLE_ERROR( cudaMemcpyAsync( host_c+i, dev_c0,
                                       N * sizeof(int),
                                       cudaMemcpyDeviceToHost,
                                       stream0 ) );
        HANDLE_ERROR( cudaMemcpyAsync( host_c+i+N, dev_c1,
                                       N * sizeof(int),
                                       cudaMemcpyDeviceToHost,
                                       stream1 ) );
    }
```
*work schdeling：*  

![bfs](/images/cuda/breadth-first.png)


# section-11：多gpu执行

## Zero-Copy Host Memory

解决了这样的问题：gpu可以直接访问cpu上的内存。


#### 前提：The frst thing we need to check is whether our device supports mapping host
memory.

代码如下：

```
if (prop.canMapHostMemory != 1) {
printf( "Device cannot map memory.\n" );
return 0;
}

```

####  若支持

则执行下面的api，实现gpu直接cpu内存的代码。

//设置flag。
0. cudaSetDeviceFlags( cudaDeviceMapHost )；


// allocate the memory on the CPU
1. cudaHostAlloc( (void**)&a, size*sizeof(float), cudaHostAllocWriteCombined | cudaHostAllocMapped );


//因为gpu端直接访存cpu端的memory，we need to call cudaHostGetDevicePointer() in order to get a valid GPU pointer for the memory.

2. cudaHostGetDevicePointer( &dev_a, a, 0 )


//kernel启动完成后，要同步。
3. cudaThreadSynchronize() ;


关于1的参数解释：

The
ﬂag cudaHostAllocMapped tells the runtime that we intend to access this
buffer from the GPU. In other words, this ﬂag is what makes our buffer zero-copy.
For the two input buffers, we specify the ﬂag cudaHostAllocWriteCombined.
This ﬂag indicates that the runtime should allocate the buffer as write-combined
with respect to the CPU cache.

the ﬂag cudaHostAllocWriteCombined will not change functionality in our application but represents an important performance enhancement for buffers that
will be read only by the GPU. However, write-combined memory can be extremely
ineffcient in scenarios where the CPU also needs to perform reads from the
buffer


#### 什么样的情况下，gpu直接访存cpu内存，不传输数据这种方式会带来性能提升

In cases where inputs and outputs are used exactly once

具体解释参看：p223.（懒得整理啦呢）


## 使用多个gpu的代码

是通过多线程方式实现的。一个线程处理一个gpu设备。具体代码参看：p225。（openmp线程或者pthread线程都可以）
cuda里面的samples程序是使用的openmp，本书所带的例子使用的是pthread。









# 疑问及解答

问题-1：

	thrust只能在main函数这种host函数中调用吗，还是可以在kernel函数中个调用？

问题-2：

	某一个变量，怎么传到kernel的参数中？
	（也需要跟数据类似吗？）



# reference
1.[CUDA by Example]()




#  TODO
1.chapter10的实验，亲自跑一下，记录一下结果。
2.调研一下gpu各种产品，型号，计算力等。
3.把所有的笔记补充完整。texture那章没读。
4.调试我的程序。


maxforall = 1139 theta0 = 47.5 phi0 = 3.21053